#!/usr/bin/env python3
"""
Example: Using Generated MCP Tools with LangChain's Official MCP Adapters

This example demonstrates how to integrate tools generated by this project
with LangChain's official MCP adapters library.

Install requirements:
    pip install langchain-mcp-adapters langgraph "langchain[openai]"

Repository: https://github.com/langchain-ai/langchain-mcp-adapters
"""

import os
import sys
import asyncio
from pathlib import Path
from typing import List

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


async def demo_stdio_server_integration():
    """
    Demo: Using tools via stdio MCP server with LangChain adapters
    """
    print("ðŸ”§ Demo 1: Stdio MCP Server Integration")
    print("=" * 60)
    
    try:
        # Import LangChain MCP adapters (requires: pip install langchain-mcp-adapters)
        from mcp import ClientSession, StdioServerParameters
        from mcp.client.stdio import stdio_client
        from langchain_mcp_adapters.tools import load_mcp_tools
        # from langgraph.prebuilt import create_react_agent  # Uncomment if you have LangGraph
        
        # Configure MCP server parameters
        server_params = StdioServerParameters(
            command="python",
            args=[str(project_root / "launch_mcp_server.py")],
        )
        
        print("ðŸ“¡ Connecting to MCP server via stdio...")
        
        # Connect to your MCP server
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                # Initialize the connection
                await session.initialize()
                print("âœ… Connected to MCP server")
                
                # Load tools using LangChain's adapter
                tools = await load_mcp_tools(session)
                print(f"ðŸ› ï¸  Loaded {len(tools)} tools:")
                
                for tool in tools:
                    print(f"   â€¢ {tool.name}: {tool.description}")
                
                # Example: Call a tool directly
                if tools:
                    print(f"\nðŸŽ¯ Testing tool: {tools[0].name}")
                    try:
                        # This is just a demo - adjust parameters based on your actual tools
                        result = await tools[0].ainvoke({})
                        print(f"ðŸ“„ Result: {result}")
                    except Exception as e:
                        print(f"âš ï¸  Tool call demo skipped: {e}")
                
                print("\nâœ… LangChain MCP Adapter integration successful!")
                
    except ImportError:
        print("âŒ LangChain MCP adapters not installed.")
        print("   Install with: pip install langchain-mcp-adapters")
    except Exception as e:
        print(f"âŒ Error: {e}")


async def demo_http_server_integration():
    """
    Demo: Using tools via HTTP MCP server with LangChain adapters
    """
    print("\nðŸŒ Demo 2: HTTP MCP Server Integration")
    print("=" * 60)
    
    try:
        from mcp import ClientSession
        from mcp.client.streamable_http import streamablehttp_client
        from langchain_mcp_adapters.tools import load_mcp_tools
        
        print("ðŸ“¡ Connecting to MCP server via HTTP...")
        print("âš ï¸  Note: This requires running an HTTP MCP server first")
        print("   Start with: python launch_mcp_server.py --transport http --port 3000")
        
        # Connect to HTTP MCP server
        async with streamablehttp_client("http://localhost:3000/mcp") as (read, write, _):
            async with ClientSession(read, write) as session:
                await session.initialize()
                print("âœ… Connected to HTTP MCP server")
                
                tools = await load_mcp_tools(session)
                print(f"ðŸ› ï¸  Loaded {len(tools)} tools via HTTP")
                
                for tool in tools:
                    print(f"   â€¢ {tool.name}")
        
    except ImportError:
        print("âŒ LangChain MCP adapters not installed.")
    except Exception as e:
        print(f"âš ï¸  HTTP server demo skipped: {e}")
        print("   Make sure to start the HTTP server first")


def demo_multi_server_client():
    """
    Demo: Using MultiServerMCPClient with generated tools
    """
    print("\nðŸš€ Demo 3: Multi-Server MCP Client")
    print("=" * 60)
    
    print("ðŸ’¡ MultiServerMCPClient Example:")
    
    example_code = '''
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent

# Configure multiple MCP servers
client = MultiServerMCPClient({
    "generated_tools": {
        "command": "python",
        "args": ["/path/to/your/launch_mcp_server.py"],
        "transport": "stdio",
    },
    "weather": {
        "url": "http://localhost:8000/mcp",
        "transport": "streamable_http",
    }
})

# Load all tools from all servers
tools = await client.get_tools()

# Create LangGraph agent with all tools
agent = create_react_agent("openai:gpt-4", tools)

# Use the agent
response = await agent.ainvoke({
    "messages": "Use the generated tools to help me"
})
'''
    
    print(example_code)


def demo_langgraph_integration():
    """
    Demo: Full LangGraph integration example
    """
    print("\nðŸ¤– Demo 4: LangGraph Agent Integration")
    print("=" * 60)
    
    print("ðŸ“š Complete LangGraph Integration Example:")
    
    example_code = '''
import asyncio
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langchain_mcp_adapters.tools import load_mcp_tools
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI

async def create_agent_with_generated_tools():
    # Your MCP server parameters
    server_params = StdioServerParameters(
        command="python",
        args=["/absolute/path/to/launch_mcp_server.py"],
    )
    
    # Connect and load tools
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()
            tools = await load_mcp_tools(session)
            
            # Create agent with your generated tools
            model = ChatOpenAI(model="gpt-4")
            agent = create_react_agent(
                model=model,
                tools=tools,
                prompt="You are a helpful assistant with access to various tools."
            )
            
            # Use the agent
            response = await agent.ainvoke({
                "messages": [
                    {"role": "user", "content": "Help me with my task using available tools"}
                ]
            })
            
            return response

# Run the agent
response = asyncio.run(create_agent_with_generated_tools())
print(response)
'''
    
    print(example_code)


def show_fastmcp_integration():
    """
    Demo: Converting your tools to FastMCP format
    """
    print("\nâš¡ Demo 5: FastMCP Integration")
    print("=" * 60)
    
    print("ðŸ”§ Converting Your Tools to FastMCP Format:")
    
    example_code = '''
# Convert your generated tools to FastMCP format
from mcp.server.fastmcp import FastMCP
from langchain_mcp_adapters.tools import to_fastmcp

# Import your generated tool
sys.path.insert(0, "generated_tools/fastflightstoolv5")
from tool import FastFlightsToolV5

# Create tool instance
flight_tool = FastFlightsToolV5()

# Convert to LangChain tool first
def create_langchain_tool_from_mcp(mcp_tool):
    from langchain_core.tools import tool
    
    @tool
    def converted_tool(**kwargs) -> str:
        """Converted MCP tool"""
        if not mcp_tool.validate(kwargs):
            return "Invalid parameters"
        return str(mcp_tool.run(kwargs))
    
    converted_tool.__name__ = mcp_tool.name
    converted_tool.__doc__ = mcp_tool.description
    return converted_tool

# Convert to LangChain tool
langchain_tool = create_langchain_tool_from_mcp(flight_tool)

# Convert to FastMCP
fastmcp_tool = to_fastmcp(langchain_tool)

# Create FastMCP server with your tool
mcp_server = FastMCP("MyGeneratedTools", tools=[fastmcp_tool])

# Run the server
if __name__ == "__main__":
    mcp_server.run(transport="stdio")
'''
    
    print(example_code)


def show_compatibility_requirements():
    """
    Show what makes tools compatible with LangChain MCP adapters
    """
    print("\nâœ… Compatibility Requirements")
    print("=" * 60)
    
    print("ðŸ” Your generated tools are compatible with LangChain MCP adapters because:")
    print("   âœ… They implement the MCP protocol correctly")
    print("   âœ… They have proper tool schemas")
    print("   âœ… They support validation and execution")
    print("   âœ… They return structured responses")
    print()
    
    print("ðŸ“‹ Required interface for LangChain compatibility:")
    interface_example = '''
class YourGeneratedTool:
    def __init__(self):
        self.name = "tool_name"
        self.description = "Tool description"
        self.parameters_schema = {
            "param1": {"type": "string", "description": "..."},
            "param2": {"type": "integer", "description": "..."}
        }
    
    def validate(self, parameters: dict) -> bool:
        # Validate parameters
        return True
    
    def run(self, parameters: dict) -> Any:
        # Execute the tool
        return result
'''
    print(interface_example)


async def main():
    """Run all demos"""
    print("ðŸš€ LangChain MCP Adapters Integration Examples")
    print("=" * 70)
    print("ðŸ“– Repository: https://github.com/langchain-ai/langchain-mcp-adapters")
    print()
    
    # Show compatibility
    show_compatibility_requirements()
    
    # Run stdio demo
    await demo_stdio_server_integration()
    
    # Run HTTP demo  
    await demo_http_server_integration()
    
    # Show multi-server example
    demo_multi_server_client()
    
    # Show LangGraph integration
    demo_langgraph_integration()
    
    # Show FastMCP integration
    show_fastmcp_integration()
    
    print("\nðŸŽ‰ All integration examples completed!")
    print("ðŸ”— For more information, visit: https://github.com/langchain-ai/langchain-mcp-adapters")


if __name__ == "__main__":
    asyncio.run(main()) 