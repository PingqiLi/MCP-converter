#!/usr/bin/env python3
"""
Example: Using Generated MCP Tools with LangChain's Official MCP Adapters

This example demonstrates how to integrate tools generated by this project
with LangChain's official MCP adapters library.

Install requirements:
    pip install langchain-mcp-adapters langgraph "langchain[openai]"

Repository: https://github.com/langchain-ai/langchain-mcp-adapters
"""

import os
import sys
import asyncio
from pathlib import Path
from typing import List

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


async def demo_stdio_server_integration():
    """
    Demo: Using tools via stdio MCP server with LangChain adapters
    """
    print("🔧 Demo 1: Stdio MCP Server Integration")
    print("=" * 60)
    
    try:
        # Import LangChain MCP adapters (requires: pip install langchain-mcp-adapters)
        from mcp import ClientSession, StdioServerParameters
        from mcp.client.stdio import stdio_client
        from langchain_mcp_adapters.tools import load_mcp_tools
        # from langgraph.prebuilt import create_react_agent  # Uncomment if you have LangGraph
        
        # Configure MCP server parameters
        server_params = StdioServerParameters(
            command="python",
            args=[str(project_root / "launch_mcp_server.py")],
        )
        
        print("📡 Connecting to MCP server via stdio...")
        
        # Connect to your MCP server
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                # Initialize the connection
                await session.initialize()
                print("✅ Connected to MCP server")
                
                # Load tools using LangChain's adapter
                tools = await load_mcp_tools(session)
                print(f"🛠️  Loaded {len(tools)} tools:")
                
                for tool in tools:
                    print(f"   • {tool.name}: {tool.description}")
                
                # Example: Call a tool directly
                if tools:
                    print(f"\n🎯 Testing tool: {tools[0].name}")
                    try:
                        # This is just a demo - adjust parameters based on your actual tools
                        result = await tools[0].ainvoke({})
                        print(f"📄 Result: {result}")
                    except Exception as e:
                        print(f"⚠️  Tool call demo skipped: {e}")
                
                print("\n✅ LangChain MCP Adapter integration successful!")
                
    except ImportError:
        print("❌ LangChain MCP adapters not installed.")
        print("   Install with: pip install langchain-mcp-adapters")
    except Exception as e:
        print(f"❌ Error: {e}")


async def demo_http_server_integration():
    """
    Demo: Using tools via HTTP MCP server with LangChain adapters
    """
    print("\n🌐 Demo 2: HTTP MCP Server Integration")
    print("=" * 60)
    
    try:
        from mcp import ClientSession
        from mcp.client.streamable_http import streamablehttp_client
        from langchain_mcp_adapters.tools import load_mcp_tools
        
        print("📡 Connecting to MCP server via HTTP...")
        print("⚠️  Note: This requires running an HTTP MCP server first")
        print("   Start with: python launch_mcp_server.py --transport http --port 3000")
        
        # Connect to HTTP MCP server
        async with streamablehttp_client("http://localhost:3000/mcp") as (read, write, _):
            async with ClientSession(read, write) as session:
                await session.initialize()
                print("✅ Connected to HTTP MCP server")
                
                tools = await load_mcp_tools(session)
                print(f"🛠️  Loaded {len(tools)} tools via HTTP")
                
                for tool in tools:
                    print(f"   • {tool.name}")
        
    except ImportError:
        print("❌ LangChain MCP adapters not installed.")
    except Exception as e:
        print(f"⚠️  HTTP server demo skipped: {e}")
        print("   Make sure to start the HTTP server first")


def demo_multi_server_client():
    """
    Demo: Using MultiServerMCPClient with generated tools
    """
    print("\n🚀 Demo 3: Multi-Server MCP Client")
    print("=" * 60)
    
    print("💡 MultiServerMCPClient Example:")
    
    example_code = '''
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent

# Configure multiple MCP servers
client = MultiServerMCPClient({
    "generated_tools": {
        "command": "python",
        "args": ["/path/to/your/launch_mcp_server.py"],
        "transport": "stdio",
    },
    "weather": {
        "url": "http://localhost:8000/mcp",
        "transport": "streamable_http",
    }
})

# Load all tools from all servers
tools = await client.get_tools()

# Create LangGraph agent with all tools
agent = create_react_agent("openai:gpt-4", tools)

# Use the agent
response = await agent.ainvoke({
    "messages": "Use the generated tools to help me"
})
'''
    
    print(example_code)


def demo_langgraph_integration():
    """
    Demo: Full LangGraph integration example
    """
    print("\n🤖 Demo 4: LangGraph Agent Integration")
    print("=" * 60)
    
    print("📚 Complete LangGraph Integration Example:")
    
    example_code = '''
import asyncio
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langchain_mcp_adapters.tools import load_mcp_tools
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI

async def create_agent_with_generated_tools():
    # Your MCP server parameters
    server_params = StdioServerParameters(
        command="python",
        args=["/absolute/path/to/launch_mcp_server.py"],
    )
    
    # Connect and load tools
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()
            tools = await load_mcp_tools(session)
            
            # Create agent with your generated tools
            model = ChatOpenAI(model="gpt-4")
            agent = create_react_agent(
                model=model,
                tools=tools,
                prompt="You are a helpful assistant with access to various tools."
            )
            
            # Use the agent
            response = await agent.ainvoke({
                "messages": [
                    {"role": "user", "content": "Help me with my task using available tools"}
                ]
            })
            
            return response

# Run the agent
response = asyncio.run(create_agent_with_generated_tools())
print(response)
'''
    
    print(example_code)


def show_fastmcp_integration():
    """
    Demo: Converting your tools to FastMCP format
    """
    print("\n⚡ Demo 5: FastMCP Integration")
    print("=" * 60)
    
    print("🔧 Converting Your Tools to FastMCP Format:")
    
    example_code = '''
# Convert your generated tools to FastMCP format
from mcp.server.fastmcp import FastMCP
from langchain_mcp_adapters.tools import to_fastmcp

# Import your generated tool
sys.path.insert(0, "generated_tools/fastflightstoolv5")
from tool import FastFlightsToolV5

# Create tool instance
flight_tool = FastFlightsToolV5()

# Convert to LangChain tool first
def create_langchain_tool_from_mcp(mcp_tool):
    from langchain_core.tools import tool
    
    @tool
    def converted_tool(**kwargs) -> str:
        """Converted MCP tool"""
        if not mcp_tool.validate(kwargs):
            return "Invalid parameters"
        return str(mcp_tool.run(kwargs))
    
    converted_tool.__name__ = mcp_tool.name
    converted_tool.__doc__ = mcp_tool.description
    return converted_tool

# Convert to LangChain tool
langchain_tool = create_langchain_tool_from_mcp(flight_tool)

# Convert to FastMCP
fastmcp_tool = to_fastmcp(langchain_tool)

# Create FastMCP server with your tool
mcp_server = FastMCP("MyGeneratedTools", tools=[fastmcp_tool])

# Run the server
if __name__ == "__main__":
    mcp_server.run(transport="stdio")
'''
    
    print(example_code)


def show_compatibility_requirements():
    """
    Show what makes tools compatible with LangChain MCP adapters
    """
    print("\n✅ Compatibility Requirements")
    print("=" * 60)
    
    print("🔍 Your generated tools are compatible with LangChain MCP adapters because:")
    print("   ✅ They implement the MCP protocol correctly")
    print("   ✅ They have proper tool schemas")
    print("   ✅ They support validation and execution")
    print("   ✅ They return structured responses")
    print()
    
    print("📋 Required interface for LangChain compatibility:")
    interface_example = '''
class YourGeneratedTool:
    def __init__(self):
        self.name = "tool_name"
        self.description = "Tool description"
        self.parameters_schema = {
            "param1": {"type": "string", "description": "..."},
            "param2": {"type": "integer", "description": "..."}
        }
    
    def validate(self, parameters: dict) -> bool:
        # Validate parameters
        return True
    
    def run(self, parameters: dict) -> Any:
        # Execute the tool
        return result
'''
    print(interface_example)


async def main():
    """Run all demos"""
    print("🚀 LangChain MCP Adapters Integration Examples")
    print("=" * 70)
    print("📖 Repository: https://github.com/langchain-ai/langchain-mcp-adapters")
    print()
    
    # Show compatibility
    show_compatibility_requirements()
    
    # Run stdio demo
    await demo_stdio_server_integration()
    
    # Run HTTP demo  
    await demo_http_server_integration()
    
    # Show multi-server example
    demo_multi_server_client()
    
    # Show LangGraph integration
    demo_langgraph_integration()
    
    # Show FastMCP integration
    show_fastmcp_integration()
    
    print("\n🎉 All integration examples completed!")
    print("🔗 For more information, visit: https://github.com/langchain-ai/langchain-mcp-adapters")


if __name__ == "__main__":
    asyncio.run(main()) 